---
title: "Project 4: Causal Inference Algorithms Evaluation"
subtitle: "Group 4"
author: "Jingbin Cao, Pin-Chun Chen, Aurore Gosmant, Weiwei Song, Zikun Zhuang"
output:
  pdf_document: default
  toc: yes
html_notebook: default
header-includes:
    - \usepackage{xcolor}
df_print: paged
---
# Basic Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F,
                      options(width = 60),
                      matrix(runif(100), ncol = 20))
```

```{r load libraries and data, message=FALSE, warning=FALSE}
#Test Branch created
if(!require("readr")){
  install.packages("readr")
}
if(!require("tidyverse")){
  install.packages("tidyverse")
}
if(!require("glmnet")){
  install.packages("glmnet")
}
if(!require("pryr")){
  install.packages("pryr")
}

library(readr)
library(tidyverse)
library(glmnet)
library(pryr)

lowDim_raw <- read_csv('../data/lowDim_dataset.csv')
highDim_raw <- read_csv('../data/highDim_dataset.csv')

lowDim <- lowDim_raw
highDim <- highDim_raw
```

# Project Overview

We will evaluate four inference algorithm in this project, Inverse Propensity Weighting (IPW) + Logistic Regression, Regression Estimate, Stratification + Logistic Regression, and Regression Adjustment + Logistic Regression. We will compute the average treatment effect (ATE) using the four algorithms on two distinct datasets (low dimension & high dimension), and we will compare their performance and computational efficiency.

# Getting Propensity Scores
We estimate the propensity scores using logistic regression:  
\[logit[Pr(T=1|X)]=\beta_0 + \beta_1x_1 + ... + \beta_px_p
\]
\[
Pr(T=1|X)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_px_p)}}
\]

```{r Getting PS, message=FALSE}
# High Dimentional Data
ps_high_estimate <- glm(data = highDim,
                        formula = A~ . -Y,
                        family=binomial())

ps_high_data <- data.frame(ps = predict(ps_high_estimate,type="response"),
                           treatment = ps_high_estimate$model$A)


# Low Dimentional Data
ps_low_estimate <- glm(data = lowDim,
                       formula = A~ . -Y,
                       family=binomial())

ps_low_data <- data.frame(ps = predict(ps_low_estimate,type="response"),
                          treatment = ps_low_estimate$model$A)

# Visualize Data
## Show PS means
(ps_means <- data.frame(high_treat_ps_mean = mean(ps_high_data[ps_high_data$treatment==1,]$ps),
                       high_control_ps_mean = mean(ps_high_data[ps_high_data$treatment==0,]$ps),
                       low_treat_ps_mean = mean(ps_low_data[ps_low_data$treatment==1,]$ps),
                       low_control_ps_mean = mean(ps_low_data[ps_low_data$treatment==0,]$ps)))

## Plot Counts
ps_high_data %>%
  mutate(treatment = ifelse(treatment == 0, "Control Group", "Treatment Group")) %>%
  ggplot(aes(x=ps))+
  ggtitle("High Dimentional Data")+
  xlab("Propensity Scores")+
  ylab("Count")+
  geom_histogram(color="white")+
  facet_wrap(~treatment)

ps_low_data %>%
  mutate(treatment = ifelse(treatment == 0, "Control Group", "Treatment Group")) %>%
  ggplot(aes(x=ps))+
  ggtitle("Low Dimential Data")+
  xlab("Propensity Scores")+
  ylab("Count")+
  geom_histogram(color="white")+
  facet_wrap(~treatment)
```


# Model 1: Inverse Propensity Weighting and Logistic Regression
\[
w_i = \frac{T_i}{\hat{e_i}}+\frac{1-T_i}{1-\hat{e_i}}
\]
where $\hat{e_i}$ is the estimated propensity score for individual $i$.  
Estimate ATE:  
\[
\hat{\triangle}_{IPW} = N^{-1}(\sum\limits_{i \in Treated}{w_iY_i}-\sum\limits_{i \in Controlled}{w_iY_i})
\]
where the first summation is from the treated group, and the second summation is from the controlled group.  

```{r}
set.seed(0)
# Define Data

# Write Algorithm
IPW <- function(df,ps){
  start <- Sys.time()
  ps['weights'] <- df$A/ps$ps+(1-df$A)/(1-ps$ps)
  treatment <- sum(ps[ps$treatment==1,]$weights*df$Y[df$A==1])
  controll <- sum(ps[ps$treatment==0,]$weights*df$Y[df$A==0])
  ATE <- (treatment-controll)/nrow(df)
  end <- Sys.time()
  runtime = end - start
  return(list(ATE=ATE,runtime=runtime))
}
# Output Performance
matrix(c(IPW(highDim,ps_high_data)$ATE,
         IPW(lowDim,ps_low_data)$ATE,
         IPW(highDim,ps_high_data)$runtime,
         IPW(lowDim,ps_low_data)$runtime),
       nrow = 2,byrow = TRUE,
       dimnames = list(c("ATE","Running Time (secs)"), c("High Dimension","Low Dimension")))
```



# Model 2: Regression Estimate

\[
\hat{\triangle}_{reg} = N^{-1}\sum\limits_{i=1}\limits^{N}(\hat{m_1}-\hat{m_0}{(X_i)})
\]
No need for Propensity Score.  

```{r}
set.seed(0)
# Write Algorithm
Regression_Estimate <- function(df){
  df_X <- df%>% select(-Y,-A)
  start <- Sys.time()
  model_0 <- glm(Y ~ ., data = subset(df[df$A==0,], select = -A))
  model_1 <- glm(Y ~ ., data = subset(df[df$A==1,], select = -A))
  Y0_pred <- predict(model_0, newdata = df%>% select(-Y,-A))
  Y1_pred <- predict(model_1, newdata = df%>% select(-Y,-A))
  ATE = 1/nrow(df) * sum(Y1_pred - Y0_pred)
  end <- Sys.time()
  runtime = end - start
  return(list(ATE = ATE, runtime = runtime))
}

# Output Performance
matrix(c(Regression_Estimate(highDim)$ATE,
         Regression_Estimate(lowDim)$ATE,
         Regression_Estimate(highDim)$runtime,
         Regression_Estimate(lowDim)$runtime),
         nrow = 2,
         byrow = TRUE,
         dimnames = list(c("ATE","Running Time (secs)"), c("High Dimension","Low Dimension")))
```

# Model 3: Stratification and Logistic Regression

\[
\hat{\triangle}_S = \sum\limits^K\limits_{j = 1} \frac{N_j}{N}(N_{1j}^{-1} \sum\limits^{N}\limits_{i=1}T_iY_iI(\hat{e}_i \in \hat{Q_j})-N_{0j}^{-1}\sum\limits^{N}\limits_{i=1}(1-T_i)Y_iI(\hat{e}_i \in \hat{Q}_j))
\]
where $K$ is the number of strata (K=5).  
$N_j$ is the number of individuals in stratum $j$.  
$N_{1j}$ is the number of "treated" individuals in stratum $j$, and $N_{0j}$ is the number of "controlled" individuals in stratum $j$.  
$\hat{Q}_j=(\hat{q}_{j-1},\hat{q}_j)$ where $\hat{q}_j$ is the jth sample quantile of the estimated propensity scores.  

Some brief introduction...

```{r}
set.seed(0)
# Define Data

# Write Algorithm
Strat <- function(){
  start = Sys.time()
  ATE =
  end = Sys.time()
  runtime = end - start
  return(list(ATE = ATE, runtime = runtime))
}
# Output Performance
matrix(c(),
       nrow = 2,
       byrow = TRUE,
       dimnames = list(c("ATE","Running Time (secs)"), c("High Dimension","Low Dimension")))
```

# Model 4: Regression Adjustment and Logistic Regression

Regress the outcome variable Y on treatment indicator variable T and the estimated propensity score. Then, the estimated coefficient on the treatment indicator variable would be an estimate of ATE.  
In this method, we regress the response variable ($Y$) with the treatment variable ($A$) and the propensity scores estimated using our model above, in this case, logistic regression. The estimated coefficient of the treatment variable ($A$) is then an estimate of the ATE.

D’Agostino (1998) and Austin (2011) compare regression adjustment with more traditional propensity score methods. One of the main advantages of the regression adjustment is in its simplicity in execution, in which one performs a somewhat basic linear regression model on two covariates and one response variable.

However, depending on the size of the dataset, this may run into computation issues as linear regression involves finding the inverse of a matrix. Additionally, regression adjustment may also not be helpful in cases where there is a strong separation between the two groups.

No such issues were present in this setup given that both datasets had a relatively small number of observations and there is no clear separation between the two groups
```{r}
set.seed(0)
# Define Data

# Write Algorithm
Reg_adj <- function(df,ps_data){
  df<- data.frame(cbind(Y=df$Y,A=df$A,ps=ps_data$ps))
  start <- Sys.time()
  m<- lm(Y ~ A+ ps, data = df)
  ATE = m$coefficients[2]
  end <- Sys.time()
  runtime = end - start
  return(list(ATE = ATE, runtime = runtime))
}
# Output Performance
matrix(c(Reg_adj(highDim,ps_high_data)$ATE,
         Reg_adj(lowDim,ps_low_data)$ATE,
         Reg_adj(highDim,ps_high_data)$runtime,
         Reg_adj(lowDim,ps_high_data)$runtime),
       nrow = 2,
       byrow = TRUE,
       dimnames = list(c("ATE","Running Time (secs)"), c("High Dimension","Low Dimension")))
```


# Model Comparisons

## True Average Treatment Effect (ATE)

```{r echo=FALSE}
set.seed(0)
true_ate <- data.frame(
                       dataset = c("Low Dim.", "High Dim."),
                       True_ATE = c(2.0901,-54.8558)
                       )
knitr::kable(true_ate)
```

Performance = squared difference of true ATE and estimated ATE
Run time (in seconds)
```{r Comparing Result, echo=FALSE}
high_dim_result <- data.frame(
                               Model = c("True ATE",
                                         "Inverse Propensity Weighting (IPW) + Logistic Regression",
                                         "Regression Estimate",
                                         "Stratification + Logistic Regression",
                                         "egression Adjustment + Logistic Regression"),

                               ATE = c(-54.8558,
                                       IPW.High.ATE ,
                                       Regression_Estimate(highDim)$ATE,
                                       Strat.High.ATE,
                                       Reg_adj.High.ATE),

                               Run_Time = c(IPW.High.ATE,
                                            Regression_Estimate(highDim)$runtime[[1]],
                                            Strat.High.ATE,
                                            Reg_adj.High.ATE),

                               Performance = c(sqrt(abs(-3-IPW.High.ATE)),
                                               sqrt(abs(-3-Regression_Estimate(hd)$ATE)),
                                               sqrt(abs(-3-Strat.High.ATE)),
                                               sqrt(abs(-3-Reg_adj.High.ATE)))
                               )

low_dim_result <- data.frame(
                               Model = c("True ATE",
                                         "Inverse Propensity Weighting (IPW) + Logistic Regression",
                                         "Regression Estimate",
                                         "Stratification + Logistic Regression",
                                         "egression Adjustment + Logistic Regression"),

                               ATE = c(2.0901,
                                       IPW.Low.ATE ,
                                       Regression_Estimate(lowDim)$ATE,
                                       Strat.Low.ATE,
                                       Reg_adj.Low.ATE),

                               Run_Time = c(IPW.Low.ATE,
                                            Regression_Estimate(lowDim)$runtime[[1]],
                                            Strat.Low.ATE,
                                            Reg_adj.Low.ATE),

                               Performance = c(sqrt(abs(-3-IPW.Low.ATE)),
                                               sqrt(abs(-3-Regression_Estimate(ld)$ATE)),
                                               sqrt(abs(-3-Strat.Low.ATE)),
                                               sqrt(abs(-3-Reg_adj.Low.ATE)))
                               )
```

## Low Dimension Dataset
```{r echo=FALSE}
knitr::kable(low_dim_result)
```

## High Dimension Dataset
```{r echo=FALSE}
knitr::kable(high_dim_result)
```

Model xxx has the best performance on Low Dimension Dataset; model xxx has the best performance on High Dimension Dataset.

# Ploting the Result
\newpage
```{r echo=FALSE}
high_results <- high_results %>%
  mutate(dim = 'high') %>%
  mutate(Model = c("IPW +\n Logistic Regression",
                   "Regression Estimate",
                   "Weighted Regression +\n Logistic Regression",
                   "Regression Adjustment +\n Logistic Regression")) %>%
  mutate(true_ATE = )
low_results <- low_results %>%
  mutate(dim = 'low') %>%
  mutate(Model = c("IPW +\n Logistic Regression",
                   "Regression Estimate",
                   "Weighted Regression +\n Logistic Regression",
                   "Regression Adjustment +\n Logistic Regression")) %>%
  mutate(true_ATE = )
results <- rbind(low_results, high_results)

#plots
ggplot(results, aes(Model, ATE, group=dim, col = dim)) +
  geom_point() +
  geom_line()+
  labs(title='ATE of Different Models',
       x='',
       y='',
       col='Dimension') +
  geom_text(aes(label=as.character(round(ATE, 3))), nudge_y = 0.2)+
  theme_light()

ggplot(results, aes(Model, Performance, group=dim, col = dim)) +
  geom_point() +
  geom_line()+
  labs(title='Performance of Different Models',
       x='',
       y='Sqrt. Error',
       col='Dimension') +
  theme_light()

ggplot(results, aes(Model, Run.Time, group=dim, col = dim)) +
  geom_point() +
  geom_line()+
  labs(title='Run Time of Different Models',
       x='',
       y='Run Time (in seconds)',
       col='Dimension') +
  theme_light()
```

# Reference

Austin, Peter C. 2011. “An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.” Multivariate Behavioral Research 46 (3): 399–424.  
Lunceford, Jared K, and Marie Davidian. 2004. “Stratification and Weighting via the Propensity Score in Estimation of Causal Treatment Effects a Comparative Study.” Statistics in Medicine 23 (19): 2937–60.  
Stuart, Elizabeth A. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 25 (1): 1.  
