---
title: "Project 4: Causal Inference Algorithms Evaluation"
subtitle: "Group 4"
author: "Jingbin Cao, Aurore Gosmant, James Smiley, Weiwei Song, Zikun Zhuang"
output:
  pdf_document: default
  toc: yes
html_notebook: default
header-includes:
    - \usepackage{xcolor}
df_print: paged
---
# Basic Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F,
                      options(width = 60),
                      matrix(runif(100), ncol = 20))
```

```{r load libraries and data, message=FALSE, warning=FALSE}
#Test Branch created
if(!require("readr")){
  install.packages("readr")
}
if(!require("tidyverse")){
  install.packages("tidyverse")
}
if(!require("glmnet")){
  install.packages("glmnet")
}
if(!require("pryr")){
  install.packages("pryr")
}

library(readr)
library(tidyverse)
library(glmnet)
library(pryr)

lowDim_raw <- read_csv('../data/lowDim_dataset.csv')
highDim_raw <- read_csv('../data/highDim_dataset.csv')

lowDim <- lowDim_raw
highDim <- highDim_raw

# Prepare Data
Y_high <-highDim$Y
X_high <-highDim%>% select(-Y, -A) %>% as.matrix
A_high <-highDim$A

Y_low <- lowDim$Y
A_low <- lowDim$A
X_low <- lowDim%>% select(-Y, -A) %>% as.matrix
```

# Project Overview

We will evaluate four inference algorithm in this project, Inverse Propensity Weighting (IPW) + Logistic Regression, Regression Estimate, Stratification + Logistic Regression, and Regression Adjustment + Logistic Regression. We will compute the average treatment effect (ATE) using the four algorithms on two distinct datasets (low dimension & high dimension), and we will compare their performance and computational efficiency. 

# Getting Propensity Scores
We estimate the propensity scores using logistic regression:  
\[logit[Pr(T=1|X)]=\beta_0 + \beta_1x_1 + ... + \beta_px_p
\]
\[
Pr(T=1|X)=\frac{1}{1+e^{-(\beta_0+\beta_1x_1+...+\beta_px_p)}}
\]

```{r Getting PS, message=FALSE}
# High Dimentional Data
ps_high_estimate <- glm(data = highDim,
                        formula = A~ . -Y,
                        family=binomial())

ps_high_data <- data.frame(ps = predict(ps_high_estimate,type="response"),
                           treatment = ps_high_estimate$model$A)


# Low Dimentional Data
ps_low_estimate <- glm(data = lowDim,
                       formula = A~ . -Y,
                       family=binomial())

ps_low_data <- data.frame(ps = predict(ps_low_estimate,type="response"),
                          treatment = ps_low_estimate$model$A)

# Visualize Data
## Show PS means
(ps_means <- data.frame(high_treat_ps_mean = mean(ps_high_data[ps_high_data$treatment==1,]$ps),
                       high_control_ps_mean = mean(ps_high_data[ps_high_data$treatment==0,]$ps),
                       low_treat_ps_mean = mean(ps_low_data[ps_low_data$treatment==1,]$ps),
                       low_control_ps_mean = mean(ps_low_data[ps_low_data$treatment==0,]$ps)))

## Plot Counts
ps_high_data %>% 
  mutate(treatment = ifelse(treatment == 0, "Control Group", "Treatment Group")) %>%
  ggplot(aes(x=ps))+
  ggtitle("High Dimentional Data")+
  xlab("Propensity Scores")+
  ylab("Count")+
  geom_histogram(color="white")+
  facet_wrap(~treatment)

ps_low_data %>% 
  mutate(treatment = ifelse(treatment == 0, "Control Group", "Treatment Group")) %>%
  ggplot(aes(x=ps))+
  ggtitle("Low Dimential Data")+
  xlab("Propensity Scores")+
  ylab("Count")+
  geom_histogram(color="white")+
  facet_wrap(~treatment)
```


# Model 1: Inverse Propensity Weighting and Logistic Regression
\[
w_i = \frac{T_i}{e_i}+\frac{1=T_i}{1-\hat{e_i}}
\] 
where $\hat{e_i}$ is the estimated propensity score for individual $i$.  
Estimate ATE:  
\[
\hat{\triangle}_{IPW} = N^{-1}(\sum\limits_{i \in Treated}{w_iY_i}-\sum\limits_{i \in Controlled}{w_iY_i})
\]
where the first summation is from the treated group, and the second summation is from the controlled group.  

Some brief introduction...
```{r}
set.seed(0)
# Define Data

# Write Algorithm
IPW <- function(){
  start = Sys.time()
  ATE =  
  end = Sys.time()
  runtime = end - start
  return(list(ATE = ATE, runtime = runtime))
}
# Output Performance
matrix(c(),
       nrow = 2,byrow = TRUE,
       dimnames = list(c("ATE","Running Time (secs)"), c("High Dimension","Low Dimension")))
```


# Model 2: Regression Estimate

\[
\hat{\triangle}_{reg} = N^{-1}\sum\limits_{i=1}\limits^{N}(\hat{m_1}-\hat{m_0}{(X_i)})
\]
No need for Propensity Score.  

Some brief introduction... 

```{r}
set.seed(0)
# Write Algorithm
Regression_Estimate <- function(df){
  df_X <- df%>% select(-Y,-A)
  start <- Sys.time()
  m0 <- glm(Y ~ ., data = subset(df[df$A==0,], select = -A))
  m1 <- glm(Y ~ ., data = subset(df[df$A==1,], select = -A))
  Y_pred_0 <- predict(m0, newdata = df_X)
  Y_pred_1 <- predict(m1, newdata = df_X)
  df <- df %>% mutate(Y_pred1 = Y_pred_1, Y_pred0 = Y_pred_0)
  ATE = 1/nrow(df) * sum(df$Y_pred1 - df$Y_pred0)
  end <- Sys.time()
  runtime = end - start
  return(list(ATE = ATE, runtime = runtime))
}

# Output Performance
matrix(c(Regression_Estimate(highDim)$ATE,
         Regression_Estimate(lowDim)$ATE,
         Regression_Estimate(highDim)$runtime,
         Regression_Estimate(lowDim)$runtime),
         nrow = 2,
         byrow = TRUE,
         dimnames = list(c("ATE","Running Time (secs)"), c("High Dimension","Low Dimension")))
```

# Model 3: Stratification and Logistic Regression

\[
\hat{\triangle}_S = \sum\limits^K\limits_{j = 1} \frac{N_j}{N}(N_{1j}^{-1} \sum\limits^{N}\limits_{i=1}T_iY_iI(\hat{e}_i \in \hat{Q_j})-N_{0j}^{-1}\sum\limits^{N}\limits_{i=1}(1-T_i)Y_iI(\hat{e}_i \in \hat{Q}_j))
\]
where $K$ is the number of strata (K=5).  
$N_j$ is the number of individuals in stratum $j$.  
$N_{1j}$ is the number of "treated" individuals in stratum $j$, and $N_{0j}$ is the number of "controlled" individuals in stratum $j$.  
$\hat{Q}_j=(\hat{q}_{j-1},\hat{q}_j)$ where $\hat{q}_j$ is the jth sample quantile of the estimated propensity scores.  

Some brief introduction...

```{r}
set.seed(0)
# Define Data

# Write Algorithm
Strat <- function(){
  start = Sys.time()
  ATE = 
  end = Sys.time()
  runtime = end - start
  return(list(ATE = ATE, runtime = runtime))
}
# Output Performance
matrix(c(),
       nrow = 2,
       byrow = TRUE,
       dimnames = list(c("ATE","Running Time (secs)"), c("High Dimension","Low Dimension")))
```

# Model 4: Regression Adjustment and Logistic Regression

Regress the outcome variable Y on treatment indicator variable T and the estimated propensity score. Then, the estimated coefficient on the treatment indicator variable would be an estimate of ATE.  

Some brief introduction...

```{r}
set.seed(0)
# Define Data

# Write Algorithm
Reg_adj <- function(){
  start <- Sys.time()
  ATE <- 
  end <- Sys.time()
  runtime <- end - start
  return(list(ATE = ATE, runtime = runtime))
}
# Output Performance
matrix(c(),
       nrow = 2,
       byrow = TRUE,
       dimnames = list(c("ATE","Running Time (secs)"), c("High Dimension","Low Dimension")))
```


# Model Comparisons

## True Average Treatment Effect (ATE)

```{r echo=FALSE}
set.seed(0)
true_ate <- data.frame(
                       dataset = c("Low Dim.", "High Dim."),
                       True_ATE = c()
                       )
knitr::kable(true_ate)
```

Performance = squared difference of true ATE and estimated ATE
Run time (in seconds)
```{r Comparing Result, echo=FALSE}
high_dim_result <- data.frame(
                               Model = c("Inverse Propensity Weighting (IPW) + Logistic Regression", 
                                         "Regression Estimate", 
                                         "Stratification + Logistic Regression", 
                                         "egression Adjustment + Logistic Regression"),
                               
                               ATE = c(IPW.High.ATE ,
                                       Regression_Estimate(hd)$ATE,
                                       Strat.High.ATE,
                                       Reg_adj.High.ATE),
                                       
                               Run_Time = c(IPW.High.ATE,
                                            Regression_Estimate(hd)$runtime[[1]],
                                            Strat.High.ATE,
                                            Reg_adj.High.ATE),
                               
                               Performance = c(sqrt(abs(-3-IPW.High.ATE)),
                                               sqrt(abs(-3-Regression_Estimate(hd)$ATE)),
                                               sqrt(abs(-3-Strat.High.ATE)),
                                               sqrt(abs(-3-Reg_adj.High.ATE)))
                               )

low_dim_result <- data.frame(
                               Model = c("Inverse Propensity Weighting (IPW) + Logistic Regression", 
                                         "Regression Estimate", 
                                         "Stratification + Logistic Regression", 
                                         "egression Adjustment + Logistic Regression"),
                               
                               ATE = c(IPW.Low.ATE ,
                                       Regression_Estimate(ld)$ATE,
                                       Strat.Low.ATE,
                                       Reg_adj.Low.ATE),
                                       
                               Run_Time = c(IPW.Low.ATE,
                                            Regression_Estimate(ld)$runtime[[1]],
                                            Strat.Low.ATE,
                                            Reg_adj.Low.ATE),
                               
                               Performance = c(sqrt(abs(-3-IPW.Low.ATE)),
                                               sqrt(abs(-3-Regression_Estimate(ld)$ATE)),
                                               sqrt(abs(-3-Strat.Low.ATE)),
                                               sqrt(abs(-3-Reg_adj.Low.ATE)))
                               )
``` 

## Low Dimension Dataset
```{r echo=FALSE}
knitr::kable(low_dim_result)
```

## High Dimension Dataset
```{r echo=FALSE}
knitr::kable(high_dim_result)
```

Model xxx has the best performance on Low Dimension Dataset; model xxx has the best performance on High Dimension Dataset.

# Ploting the Result
\newpage
```{r echo=FALSE}
high_results <- high_results %>% 
  mutate(dim = 'high') %>% 
  mutate(Model = c("IPW +\n Logistic Regression", 
                   "Regression Estimate",
                   "Weighted Regression +\n Logistic Regression", 
                   "Regression Adjustment +\n Logistic Regression")) %>% 
  mutate(true_ATE = )
low_results <- low_results %>% 
  mutate(dim = 'low') %>% 
  mutate(Model = c("IPW +\n Logistic Regression", 
                   "Regression Estimate",
                   "Weighted Regression +\n Logistic Regression", 
                   "Regression Adjustment +\n Logistic Regression")) %>% 
  mutate(true_ATE = )
results <- rbind(low_results, high_results)

#plots
ggplot(results, aes(Model, ATE, group=dim, col = dim)) +
  geom_point() +
  geom_line()+
  labs(title='ATE of Different Models',
       x='',
       y='',
       col='Dimension') +
  geom_text(aes(label=as.character(round(ATE, 3))), nudge_y = 0.2)+
  theme_light()

ggplot(results, aes(Model, Performance, group=dim, col = dim)) +
  geom_point() +
  geom_line()+
  labs(title='Performance of Different Models',
       x='',
       y='Sqrt. Error',
       col='Dimension') +
  theme_light()

ggplot(results, aes(Model, Run.Time, group=dim, col = dim)) +
  geom_point() +
  geom_line()+
  labs(title='Run Time of Different Models',
       x='',
       y='Run Time (in seconds)',
       col='Dimension') +
  theme_light()
```

# Reference

Austin, Peter C. 2011. “An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies.” Multivariate Behavioral Research 46 (3): 399–424.  
Lunceford, Jared K, and Marie Davidian. 2004. “Stratification and Weighting via the Propensity Score in Estimation of Causal Treatment Effects a Comparative Study.” Statistics in Medicine 23 (19): 2937–60.  
Stuart, Elizabeth A. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 25 (1): 1.  

